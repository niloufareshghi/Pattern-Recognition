{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYykSgbKhnWQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk #NATURAL LANGUAGE PROCESSING TOOLKIT\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import json\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = {}\n",
        "for line in open('tweets.json', 'r'):\n",
        "    tweet = (json.loads(line))\n",
        "    tweets[tweet['id']] = tweet['text']"
      ],
      "metadata": {
        "id": "omyZDbX0kovd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_file = open(\"initial_centroids.txt\", \"r\")\n",
        "file_content = txt_file.read()\n",
        "centroids_init = file_content.split(\",\\n\")\n",
        "txt_file.close()\n",
        "centroids_init = [int(c) for c in centroids_init]\n",
        "centroids_init"
      ],
      "metadata": {
        "id": "JVAWopAqnwEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccardDistance(setA, setB):\n",
        "  return 1 - float(len(setA.intersection(setB))) / float(len(setA.union(setB)))"
      ],
      "metadata": {
        "id": "LNpDeL6qrcDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tweet_text):\n",
        "  words = wordpunct_tokenize(tweet_text.translate(str.maketrans('', '', string.punctuation)))\n",
        "  return set(words)"
      ],
      "metadata": {
        "id": "DJ66r6irvtCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_cluster(data, centroids):\n",
        "    assignments = {}\n",
        "\n",
        "    for key, val in data.items():\n",
        "        dist_point_clust = []\n",
        "\n",
        "        for centroid in centroids:\n",
        "            words_centroid = tokenize(tweets[centroid])\n",
        "            words_data = tokenize(val)\n",
        "            d_clust = jaccardDistance(words_centroid, words_data)\n",
        "            dist_point_clust.append(d_clust)\n",
        "        \n",
        "        assignment = centroids[np.argmin(dist_point_clust)]\n",
        "        assignments[key] = assignment\n",
        "\n",
        "    return assignments"
      ],
      "metadata": {
        "id": "gcFd3QW_345u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_centroids(data, centroids, assignments):\n",
        "    new_centroids = []\n",
        "    for centroid in centroids:\n",
        "        pt_cluster = []\n",
        "        for key, val in data.items():\n",
        "                if (assignments[key] == centroid):\n",
        "                    pt_cluster.append(key)        \n",
        "        new_centroids.append(pt_cluster[len(pt_cluster)//2])\n",
        "\n",
        "    return new_centroids"
      ],
      "metadata": {
        "id": "-10A5MVvSjTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def errors(data, assignments, centroids):\n",
        "    errors = []\n",
        "    \n",
        "    for key, val in data.items():\n",
        "        centroid = assignments[key]\n",
        "    \n",
        "        words_centroid = tokenize(data[centroid])\n",
        "        words_data = tokenize(val)\n",
        "        error = jaccardDistance(words_centroid, words_data)\n",
        "        \n",
        "        errors.append(error**2)\n",
        "        \n",
        "    total_error = sum(errors)\n",
        "    \n",
        "    return total_error"
      ],
      "metadata": {
        "id": "mF3vSAc-TQeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KMeans(data, K, max_iter = 100, tol = pow(10,-3)):\n",
        "    it = -1\n",
        "    es = []\n",
        "    assignments = []\n",
        "    \n",
        "    centroids = centroids_init\n",
        "   \n",
        "    while (len(es)<=1 or (it < max_iter and np.absolute(es[it] - es[it-1])/es[it-1] >= tol)):\n",
        "        it += 1\n",
        "        assignments = assign_cluster(data, centroids)\n",
        "        \n",
        "        centroids = new_centroids(data, centroids, assignments)\n",
        "        \n",
        "        kmeans_error = errors(data, assignments, centroids)\n",
        "        es.append(kmeans_error)        \n",
        "        \n",
        "     \n",
        "    return (assignments, centroids, data, it+1)"
      ],
      "metadata": {
        "id": "mccA69YbUETl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_centroids(data, k, random_state=42):\n",
        "    \n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    centroids = [data[list(data.keys())[0]]]\n",
        "\n",
        "    for _ in range(25):\n",
        "        dist_sq = np.array([min([jaccardDistance(tokenize(data[c]), tokenize(x)) for c in centroids]) for x in data.values()])\n",
        "        key_seq = \n",
        "        probs = dist_sq/dist_sq.sum()\n",
        "        cumulative_probs = probs.cumsum()\n",
        "        r = np.random.rand()\n",
        "        \n",
        "        for j, p in enumerate(cumulative_probs):\n",
        "            if r < p:\n",
        "                i = j\n",
        "                break\n",
        "        \n",
        "        centroids.append(list(data.keys())[i])\n",
        "\n",
        "    return np.array(centroids)\n"
      ],
      "metadata": {
        "id": "O6UVpZOpekCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}